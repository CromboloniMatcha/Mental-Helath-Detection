{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mental Health Detection - Main Script\n",
    "\n",
    "Copyright (C) 2025 Yanuar Noor Wicaksono A.K.A Christianus Yanuar\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License along with this program. If not, see https://www.gnu.org/licenses/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40907f6-6898-4ffe-a33c-72cd336cefc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from -r requirements.txt (line 3)) (1.7.2)\n",
      "Collecting nltk (from -r requirements.txt (line 4))\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: gensim in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from -r requirements.txt (line 5)) (4.4.0)\n",
      "Requirement already satisfied: jupyter in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from -r requirements.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nltk->-r requirements.txt (line 4)) (8.3.0)\n",
      "Collecting regex>=2021.8.3 (from nltk->-r requirements.txt (line 4))\n",
      "  Downloading regex-2025.10.23-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nltk->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from gensim->-r requirements.txt (line 5)) (7.4.1)\n",
      "Requirement already satisfied: notebook in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (7.4.7)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (6.30.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter->-r requirements.txt (line 6)) (4.4.10)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from smart_open>=1.8.1->gensim->-r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from click->nltk->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (9.5.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (5.14.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.8.5)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r requirements.txt (line 6)) (311)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (3.0.15)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (2.0.5)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (2.28.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (80.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.23.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.25.1)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.32.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.15.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (25.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.28.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (6.0.3)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (4.14.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.21.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.5.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.23)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\chris\\ar\\venv_312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.3)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.5 MB/s  0:00:01\n",
      "Downloading regex-2025.10.23-cp312-cp312-win_amd64.whl (276 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   ---------------------------------------- 2/2 [nltk]\n",
      "\n",
      "Successfully installed nltk-3.9.2 regex-2025.10.23\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\chris\\AR\\venv_312\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c729ce3e-57c9-4eb6-bd3c-be814497a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengunduh paket NLTK yang diperlukan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paket NLTK (punkt, punkt_tab, wordnet, averaged_perceptron_tagger, stopwords) berhasil diunduh/sudah ada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BAGIAN 1: INSTALASI DAN IMPORT LIBRARY\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Untuk Preprocessing dengan NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag  # Ini untuk Part-of-Speech (POS) Tagging\n",
    "\n",
    "# Untuk Metode 1: Frequency-Based\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Untuk Metode 2: Prediction-Based\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# --- Mengunduh data yang diperlukan oleh NLTK ---\n",
    "# Jalankan ini sekali saja.\n",
    "print(\"Mengunduh paket NLTK yang diperlukan...\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True) # Untuk tokenization\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('wordnet', quiet=True) # Untuk lemmatization\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True) # Untuk POS tagging\n",
    "    nltk.download('stopwords', quiet=True) # Untuk stop words\n",
    "    print(\"Paket NLTK (punkt, punkt_tab, wordnet, averaged_perceptron_tagger, stopwords) berhasil diunduh/sudah ada.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saat mengunduh paket NLTK: {e}\")\n",
    "    print(\"Pastikan Anda terhubung ke internet.\")\n",
    "    \n",
    "# Siapkan stop words dan lemmatizer\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1a3486-4edb-4097-973e-8837b7b81c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'reddit_mentalhealth_sample (1).csv' berhasil dimuat.\n",
      "Bentuk data awal: (700, 2)\n",
      "Bentuk data setelah menghapus NaN: (700, 2)\n",
      "Bentuk data setelah menghapus duplikat: (700, 2)\n",
      "\n",
      "Contoh data (5 baris pertama):\n",
      "  subreddit                                            content\n",
      "0   Anxiety  My #1 biggest fear is death. Losing my conscio...\n",
      "1   Anxiety  Trying my 4th med out soon. The first 3 anxiet...\n",
      "2   Anxiety  Nauseated when thinking about a holiday How ca...\n",
      "3   Anxiety  Angerxiety? Does anyone else experience anger ...\n",
      "4   Anxiety  No idea how to deal with new anxiety For as lo...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BAGIAN 2: MEMUAT DAN MEMBERSIHKAN DATA\n",
    "# -------------------------------------------------------------------\n",
    "file_path = 'reddit_mentalhealth_sample (1).csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset '{file_path}' berhasil dimuat.\")\n",
    "    print(f\"Bentuk data awal: {df.shape}\")\n",
    "\n",
    "    # 1. Ganti string kosong dengan NaN (jika ada)\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "    # 2. Hapus baris yang memiliki nilai NaN di 'content' atau 'subreddit'\n",
    "    df_cleaned = df.dropna(subset=['content', 'subreddit'])\n",
    "    print(f\"Bentuk data setelah menghapus NaN: {df_cleaned.shape}\")\n",
    "\n",
    "    # 3. Hapus duplikat (jika ada)\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"Bentuk data setelah menghapus duplikat: {df_cleaned.shape}\")\n",
    "\n",
    "    # 4. Tampilkan 5 baris pertama\n",
    "    print(\"\\nContoh data (5 baris pertama):\")\n",
    "    print(df_cleaned.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{file_path}' tidak ditemukan.\")\n",
    "    print(\"Pastikan file tersebut berada di folder yang sama dengan notebook Anda.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19cd6560-8ab6-4237-941c-ed110c17ebda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai proses preprocessing lanjutan (NLTK POS Tagging & Lemmatization)...\n",
      "Preprocessing selesai.\n",
      "\n",
      "Contoh hasil preprocessing:\n",
      "                                             content  \\\n",
      "0  My #1 biggest fear is death. Losing my conscio...   \n",
      "1  Trying my 4th med out soon. The first 3 anxiet...   \n",
      "2  Nauseated when thinking about a holiday How ca...   \n",
      "3  Angerxiety? Does anyone else experience anger ...   \n",
      "4  No idea how to deal with new anxiety For as lo...   \n",
      "\n",
      "                                   processed_content  \n",
      "0  big fear death lose consciousness since little...  \n",
      "1  try med first anxiety med make bad buspar zolo...  \n",
      "2  nauseate think holiday deal nausea talk trip m...  \n",
      "3  angerxiety anyone else experience anger alongs...  \n",
      "4  idea deal new anxiety long remember anxious pe...  \n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BAGIAN 3: ADVANCED PREPROCESSING (CLEANING + POS TAGGING NLTK)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Ini adalah fungsi bantuan untuk mengubah format tag NLTK\n",
    "# agar sesuai dengan format yang diterima oleh lemmatizer.\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    \"\"\"Konversi tag NLTK ke tag WordNet.\"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # Default-nya adalah NOUN jika tidak dikenali\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Daftar POS Tag yang ingin kita simpan\n",
    "# NOUN, VERB, ADJ (Kata Benda, Kata Kerja, Kata Sifat)\n",
    "KEPT_POS_TAGS = [wordnet.NOUN, wordnet.VERB, wordnet.ADJ]\n",
    "\n",
    "def advanced_preprocess_nltk(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk membersihkan dan memfilter teks menggunakan NLTK.\n",
    "    \"\"\"\n",
    "    # 1. Hapus tanda baca, angka, dan karakter khusus\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    \n",
    "    # 2. Ubah jadi huruf kecil\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Tokenisasi (memecah teks menjadi kata-kata)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Dapatkan POS Tag untuk setiap token\n",
    "    # Hasilnya: [('my', 'PRP$'), ('fear', 'NN'), ('is', 'VBZ'), ('death', 'NN')]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    clean_lemmas = []\n",
    "    # 5. Lakukan Lemmatization, Stop Word Removal, dan POS Tag Filtering\n",
    "    for token, tag in tagged_tokens:\n",
    "        \n",
    "        # Konversi tag NLTK ke tag WordNet\n",
    "        wordnet_tag = get_wordnet_pos(tag)\n",
    "        \n",
    "        # Periksa 3 kondisi:\n",
    "        # - Token BUKAN stop word\n",
    "        # - Panjang token > 2 (menghapus kata seperti 'I', 'a')\n",
    "        # - POS tag-nya ada di daftar yang kita inginkan\n",
    "        if (token not in STOP_WORDS) and (len(token) > 2) and (wordnet_tag in KEPT_POS_TAGS):\n",
    "            \n",
    "            # Ambil bentuk dasarnya (lemma) menggunakan tag POS yang benar\n",
    "            lemma = lemmatizer.lemmatize(token, pos=wordnet_tag)\n",
    "            clean_lemmas.append(lemma)\n",
    "            \n",
    "    # 6. Mengembalikan daftar token/lemma yang sudah bersih\n",
    "    return clean_lemmas\n",
    "\n",
    "\n",
    "print(\"Memulai proses preprocessing lanjutan (NLTK POS Tagging & Lemmatization)...\")\n",
    "\n",
    "# 'processed_tokens' akan berisi list dari token\n",
    "df_cleaned['processed_tokens'] = df_cleaned['content'].apply(advanced_preprocess_nltk)\n",
    "\n",
    "# 'processed_content' akan berisi string yang sudah digabung kembali\n",
    "df_cleaned['processed_content'] = df_cleaned['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "print(\"Preprocessing selesai.\")\n",
    "print(\"\\nContoh hasil preprocessing:\")\n",
    "print(df_cleaned[['content', 'processed_content']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f404999-0d90-4597-aaf3-63c2ce2db3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 700\n",
      "Jumlah data training: 560\n",
      "Jumlah data testing: 140\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BAGIAN 4: MEMBAGI DATA (TRAIN/TEST SPLIT)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "X = df_cleaned['processed_content'] # Fitur (teks yang sudah bersih)\n",
    "y = df_cleaned['subreddit']         # Label (target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Total data: {len(df_cleaned)}\")\n",
    "print(f\"Jumlah data training: {len(X_train)}\")\n",
    "print(f\"Jumlah data testing: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c0b846-ea9c-48dd-bcca-9ac47496412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bentuk matriks TF-IDF (data training): (560, 5000)\n",
      "Bentuk matriks TF-IDF (data testing): (140, 5000)\n",
      "Artinya: 560 dokumen dan 5000 fitur (kata).\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# METODE 1: FREQUENCY-BASED (TF-IDF)\n",
    "# -------------------------------------------------------------------\n",
    "# TfidfVectorizer akan mengubah teks menjadi matriks angka\n",
    "# berdasarkan skor Term Frequency-Inverse Document Frequency.\n",
    "# Inisialisasi TfidfVectorizer\n",
    "# max_features=5000 berarti kita hanya ambil 5000 kata paling penting\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# \"Fit\" (belajar) kosakata HANYA dari data training\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# \"Transform\" (ubah) data training dan testing menjadi matriks TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"\\nBentuk matriks TF-IDF (data training):\", X_train_tfidf.shape)\n",
    "print(\"Bentuk matriks TF-IDF (data testing):\", X_test_tfidf.shape)\n",
    "print(f\"Artinya: {X_train_tfidf.shape[0]} dokumen dan {X_train_tfidf.shape[1]} fitur (kata).\")\n",
    "\n",
    "# Data 'X_train_tfidf' dan 'X_test_tfidf' ini sudah siap\n",
    "# dimasukkan ke model Machine Learning (misal: LogisticRegression, RandomForest, dll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864a68d2-a45c-469b-bb1b-4a08f52bb81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melatih model Word2Vec...\n",
      "Model Word2Vec selesai dilatih.\n",
      "\n",
      "Vektor untuk kata 'anxiety':\n",
      " [-0.2224123   0.20925497 -0.2845556   0.45667154  0.16534568 -0.6691461\n",
      "  0.34463271  0.7962702  -0.45252115  0.01727195]\n",
      "\n",
      "Kata yang mirip dengan 'anxiety':\n",
      " [('take', 0.9996471405029297), ('self', 0.9996382594108582), ('make', 0.9996116161346436), ('issue', 0.9996101260185242), ('bipolar', 0.9996057152748108)]\n",
      "\n",
      "Membuat vektor dokumen (merata-ratakan vektor kata)...\n",
      "Bentuk matriks Word2Vec (data training): (560, 100)\n",
      "Bentuk matriks Word2Vec (data testing): (140, 100)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# METODE 2: PREDICTION-BASED (WORD2VEC)\n",
    "# -------------------------------------------------------------------\n",
    "# Langkah 1: Kita harus \"melatih\" model Word2Vec pada data kita.\n",
    "# Langkah 2: Kita ubah setiap *dokumen* (postingan) menjadi satu vektor\n",
    "#            dengan cara merata-ratakan vektor dari semua kata di dalamnya.\n",
    "\n",
    "# --- Langkah 2.1: Melatih Model Word2Vec ---\n",
    "# Input untuk Word2Vec adalah list of list of tokens.\n",
    "# Kita sudah punya ini di 'df_cleaned['processed_tokens']'\n",
    "corpus_tokens = df_cleaned['processed_tokens'].tolist()\n",
    "\n",
    "# Latih model\n",
    "# vector_size=100 -> setiap kata akan direpresentasikan sebagai vektor 100-dimensi\n",
    "# window=5 -> melihat 5 kata sebelum dan 5 kata sesudah saat belajar konteks\n",
    "# min_count=5 -> abaikan kata yang muncul kurang dari 5 kali\n",
    "print(\"Melatih model Word2Vec...\")\n",
    "w2v_model = Word2Vec(sentences=corpus_tokens, vector_size=100, window=5, min_count=5, workers=4)\n",
    "print(\"Model Word2Vec selesai dilatih.\")\n",
    "\n",
    "# Contoh: Melihat vektor untuk satu kata\n",
    "try:\n",
    "    print(\"\\nVektor untuk kata 'anxiety':\\n\", w2v_model.wv['anxiety'][:10]) # tampilkan 10 dimensi pertama\n",
    "except KeyError:\n",
    "    print(\"\\nKata 'anxiety' tidak ada di vocabulary (mungkin karena min_count).\")\n",
    "\n",
    "# Contoh: Melihat kata yang mirip\n",
    "try:\n",
    "    print(\"\\nKata yang mirip dengan 'anxiety':\\n\", w2v_model.wv.most_similar('anxiety', topn=5))\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "# --- Langkah 2.2: Membuat Vektor Dokumen (Document Vectors) ---\n",
    "# Kita buat fungsi untuk merata-ratakan vektor kata dalam satu dokumen\n",
    "def create_document_vector(tokens, model, vector_size=100):\n",
    "    # Buat vektor nol sebagai dasar\n",
    "    doc_vector = np.zeros(vector_size)\n",
    "    word_count = 0\n",
    "    \n",
    "    # Ambil vocabulary dari model\n",
    "    vocab = set(model.wv.index_to_key)\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in vocab:\n",
    "            doc_vector += model.wv[word]\n",
    "            word_count += 1\n",
    "            \n",
    "    # Ambil rata-ratanya\n",
    "    if word_count > 0:\n",
    "        doc_vector /= word_count\n",
    "        \n",
    "    return doc_vector\n",
    "\n",
    "print(\"\\nMembuat vektor dokumen (merata-ratakan vektor kata)...\")\n",
    "# Terapkan fungsi ini ke data training dan testing\n",
    "# Kita harus menggunakan 'processed_tokens' dari data split\n",
    "X_train_tokens = df_cleaned.loc[X_train.index, 'processed_tokens']\n",
    "X_test_tokens = df_cleaned.loc[X_test.index, 'processed_tokens']\n",
    "\n",
    "X_train_w2v = np.array([create_document_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([create_document_vector(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "\n",
    "print(\"Bentuk matriks Word2Vec (data training):\", X_train_w2v.shape)\n",
    "print(\"Bentuk matriks Word2Vec (data testing):\", X_test_w2v.shape)\n",
    "\n",
    "# Data 'X_train_w2v' dan 'X_test_w2v' ini juga sudah siap\n",
    "# dimasukkan ke model Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4584078-1c34-409a-a2c1-74638248986f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_312)",
   "language": "python",
   "name": "venv_312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
